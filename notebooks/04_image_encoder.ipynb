{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7618984e-1d8f-4d53-89e6-7be053bc9f3d",
   "metadata": {},
   "source": [
    "# CNN-Based Satellite Image Feature Extraction\n",
    "\n",
    "This notebook encodes satellite images into fixed length numerical representations\n",
    "using a pretrained Convolutional Neural Network (CNN). These embeddings capture\n",
    "high level visual neighborhood context and are later fused with tabular features\n",
    "for multimodal property valuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3aca97-c0da-4039-ac20-ecbd3f03fb46",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "We import PyTorch, TorchVision, and supporting libraries required for dataset\n",
    "construction, image preprocessing, and CNN-based feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e97ad3-b469-4836-89f8-40f9d2931150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093cea9-a202-4daf-acf3-f26897350d5b",
   "metadata": {},
   "source": [
    "## 2. Loading Sampled Training Data\n",
    "\n",
    "Satellite image embeddings are extracted only for the stratified subset of training\n",
    "data created during preprocessing. This ensures computational efficiency while\n",
    "maintaining representation across the price distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "885028c9-3eca-4e01-94b1-5d73f71130a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 21)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../data/processed/train_sampled.csv\")\n",
    "IMG_DIR = \"../data/images/train\"\n",
    "\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b1969-7d58-4b9c-8c18-0f4c197be603",
   "metadata": {},
   "source": [
    "The dataset size confirms that embeddings will be generated for all sampled properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ead147-7361-4789-9f58-76d4033da8f9",
   "metadata": {},
   "source": [
    "## 3. Image Preprocessing\n",
    "\n",
    "Satellite images are resized and normalized using ImageNet statistics.\n",
    "This preprocessing matches the requirements of the pretrained CNN and\n",
    "ensures consistent feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9eced5-1cc1-49f2-a6db-431bda65266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8ec13-ffab-4265-9b76-a6f2927b5ef5",
   "metadata": {},
   "source": [
    "## 4. Custom PyTorch Dataset\n",
    "\n",
    "A custom Dataset class is defined to load satellite images and their corresponding\n",
    "property IDs. The dataset returns images only (no labels), as the CNN is used purely\n",
    "as a feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f0497b5-1474-4efc-9a40-2ba0718b385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{row['id']}.png\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, row[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6072f2-7ce3-413c-85cd-e4eb91a88369",
   "metadata": {},
   "source": [
    "## 5. DataLoader Configuration\n",
    "\n",
    "Images are loaded in batches without shuffling to preserve the mapping between\n",
    "image embeddings and property IDs. Single threaded loading is used to ensure\n",
    "compatibility with macOS environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362c55fc-3433-4d0a-9c8c-0fb611b04e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "dataset = SatelliteDataset(\n",
    "    train_df,\n",
    "    IMG_DIR,\n",
    "    transform=image_transforms\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,    # DO NOT SHUFFLE\n",
    "    num_workers=0     # REQUIRED on macOS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ea0dde-1e28-46d1-aec0-f841f46c9110",
   "metadata": {},
   "source": [
    "## 6. Pretrained CNN Feature Extractor\n",
    "\n",
    "We use a pretrained ResNet-18 model as a fixed feature extractor.\n",
    "The final classification layer is removed, and all parameters are frozen to\n",
    "prevent training and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0edc467-d8d1-4247-8b9a-4cbec3f3e0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = nn.Identity()  # remove classifier\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea740a-8690-4ee5-86da-906fae6aa125",
   "metadata": {},
   "source": [
    "## 7. Device Configuration\n",
    "\n",
    "The model is moved to GPU if available; otherwise, computation is performed on CPU.\n",
    "The model is set to evaluation mode to disable dropout and batch normalization updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fde2e61-80c9-469d-beab-b44c204d824e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f1f56-13e2-4030-9000-7ca8c9314516",
   "metadata": {},
   "source": [
    "## 8. Extracting Image Embeddings\n",
    "\n",
    "Satellite images are passed through the CNN in batches to obtain 512-dimensional\n",
    "feature vectors. Gradients are disabled to improve efficiency and ensure\n",
    "deterministic inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe6f41bd-aac5-4abc-9372-7a3ae265b025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 157/157 [01:42<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "all_embeddings = []\n",
    "all_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, ids in tqdm(loader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        features = resnet(images)     # (B, 512)\n",
    "        features = features.cpu().numpy()\n",
    "\n",
    "        all_embeddings.append(features)\n",
    "        all_ids.extend(ids.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b681fa-943d-42c6-8dde-3152eb18dae3",
   "metadata": {},
   "source": [
    "Each satellite image is encoded into a fixed length embedding that captures\n",
    "high level spatial and visual characteristics of the surrounding neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0e24a3-c32b-44c2-8157-bf15605b9f09",
   "metadata": {},
   "source": [
    "## 9. Saving Encoded Features\n",
    "\n",
    "The extracted image embeddings and corresponding property IDs are saved to disk\n",
    "for reuse in downstream multimodal modeling without reprocessing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22675595-b1e7-4844-b8bb-9f8248a3cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (5000, 512)\n",
      "IDs shape: (5000,)\n"
     ]
    }
   ],
   "source": [
    "image_embeddings = np.vstack(all_embeddings)\n",
    "image_ids = np.array(all_ids)\n",
    "\n",
    "np.save(\"../data/processed/image_embeddings.npy\", image_embeddings)\n",
    "np.save(\"../data/processed/image_ids.npy\", image_ids)\n",
    "\n",
    "print(\"Embeddings shape:\", image_embeddings.shape)\n",
    "print(\"IDs shape:\", image_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e7dca-1de5-4d9e-a2d5-72ce231206a9",
   "metadata": {},
   "source": [
    "The final embedding matrix confirms successful encoding of all sampled images.\n",
    "Each property is represented by a 512-dimensional visual feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c70945-fd3a-4267-b6ac-6911adec8474",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook converts raw satellite images into compact numerical embeddings\n",
    "using a pretrained CNN. These embeddings encode neighborhood level visual context\n",
    "and serve as the image modality input for the multimodal property valuation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f970f2c-5cc0-476c-a6e5-f75a78cc7730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
